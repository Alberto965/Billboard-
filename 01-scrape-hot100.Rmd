---
title: "Hot 100 Scrape"
output:
  html_document:
    theme: flatly
    toc: true
    toc_depth: 3
    toc_float: true
    df_print: paged
knit: (function(inputFile, encoding) { rmarkdown::render(inputFile, encoding = encoding, output_dir = "../docs") })
---


> THIS IS DEPRECATED. Instead use 01-scrape-charts.



## Goals

1. I want to scrape the current [Billboard Hot 100](https://www.billboard.com/charts/hot-100/) chart and save it as a separate file with the current date.
2. I want to be able to target a specific date to pull that chart down

Compilations of the files will likely be in a different script.

This file is also used to test Github Actions scrapes based on crons. This is already under development with the `scrape_hot100.R` script.

## Setup

```{r setup}
library(tidyverse)
library(rvest)
library(lubridate)
library(here)
```


## Date options

I set up some flags to get a specific date or the current date.

- `F` pulls the current chart
- `T` pulls the chart that is noted in the `edition_request` object

```{r flags}
edition_request <- "2022-02-26"
edition_flag <- F

edition_current <- ""
if (edition_flag == T) edition_page <- edition_request else edition_page <- edition_current
```

## Get the page

Get the content of the page:

```{r urls-scrape}
# gets the page
scrape_url <- paste(
  "https://www.billboard.com/charts/hot-100/",
  edition_page,
  sep = ""
  )

scrape_url

first_scrape <- read_html(scrape_url)
first_scrape
```


### Get page date element

I get this date to match the real date of the data vs one that we may have asked above since pages will redirect.

```{r get-}
data_date <- first_scrape %>% 
  html_element("div#chart-date-picker") %>% 
  html_attr("data-date")

data_date
```

Create year variable for directory structure.

```{r}
chart_year <- ymd(data_date) %>% year()

# peek at it
chart_year
```


```{r}
# gets the billboard list
second_scrape <- first_scrape %>% 
  html_elements("ul.o-chart-results-list-row")

# gets the text from within the list items
all_lines <- second_scrape %>% 
  html_text2()

# converts that text from a list to a tibble
lines_tibble <- all_lines %>% as_tibble()

lines_tibble
```

## Separate the lines

Each row in the tibble has all the results of the charts, but separated by `\n`.

There is some trash in some of the entries when a song is "new" or "re-entry". There may be more. (I'm cleaning out that data for now, but some day perhaps that is worth noting in a new column. May be hard to get historical data on, though.)

```{r}
lines_cleaned <- lines_tibble %>% 
  mutate(
    data_cleaned = str_remove_all(value, " NEW\n|NEW"),
    data_cleaned = str_remove_all(data_cleaned, " RE- ENTRY\n|RE- ENTRY")
  ) %>% 
  select(data_cleaned, value)
```

Separate the data into columns.

```{r}
lines_separated <- lines_cleaned %>% separate(
  col = data_cleaned,
  sep = "\n",
  into = c(
    "current_week",
    "title",
    "performer",
    "last_week",
    "peak_pos",
    "wks_on_chart"
    )
  ) %>% 
  select(-value)

lines_separated
```

## Add a date

```{r}
dated_data <- lines_separated %>% 
  mutate(chart_week = data_date) %>% 
  select(chart_week, everything())

dated_data
```

## Testing files

Testing this concept: I need to throw an error somehow if there are not 100 rows. Preferably I would get a message to slack or email. This has not been implemented in the scraping script.

```{r}
if (dated_data %>% nrow == 100) {"yes"} else {"no"}
```

## Export

Here we create a writing path using the chart year, then test if it exists. If not, we create it. Then we write the file based on the chart date.

```{r}
folder_path <- paste("data-download/hot100-scraped/", chart_year, "/", sep = "")

if (!dir.exists(here(folder_path))) {dir.create(here(folder_path))}

dated_data %>% write_csv(paste(folder_path, data_date, ".csv", sep = ""))
```

## Import check

Just making sure the file wrote and then can be opened.

```{r}
read_csv(paste(folder_path, data_date, ".csv", sep = ""))
```

